{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats Dogs CNN Classifier\n",
    "###### Kaggle link: https://www.kaggle.com/c/dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow for obvious reasons\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for data manipulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob for getting file names\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "n_epochs = 200\n",
    "n_batch = 128\n",
    "buffer_size = 1024\n",
    "one_hot_cat = np.array([0.0, 1.0])\n",
    "one_hot_dog = np.array([1.0, 0.0])\n",
    "resize_shape = [128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file names\n",
    "filenames = np.array(glob.glob(\"./dataset/train/*.jpg\"), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total size of dataset\n",
    "n_samples = len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for labels\n",
    "labels = np.zeros(shape=(n_samples, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label vector\n",
    "for counter, image_name in enumerate(filenames):\n",
    "    if 'cat' in image_name:\n",
    "        labels[counter] = one_hot_cat\n",
    "    elif 'dog' in image_name:\n",
    "        labels[counter] = one_hot_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(filenames, labels, test_size=0.1, random_state=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for input\n",
    "X = tf.placeholder(tf.float32, [None, 128, 128, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for label\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the image parser function\n",
    "def _parser_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string)\n",
    "    image_resized = tf.image.resize_images(image, size=resize_shape)\n",
    "    image_resized = image_resized / 255.0\n",
    "    return image_resized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input pipeline using tf.data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "dataset = dataset.apply(tf.contrib.data.map_and_batch(map_func=_parser_function, batch_size=n_batch))\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch iterators\n",
    "input_mini_batch, label_mini_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test pipeline using tf.data\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=buffer_size)\n",
    "test_dataset = test_dataset.apply(tf.contrib.data.map_and_batch(map_func=_parser_function, batch_size=n_batch))\n",
    "test_iterator = test_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch iterators\n",
    "X_test_mini_batch, y_test_mini_batch = test_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights and biases with a small amount of noise\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution and pooling layers\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights and biases\n",
    "# convolutional layers\n",
    "W_conv1 = weight_variable([5,5,3,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,32])\n",
    "b_conv2 = bias_variable([32])\n",
    "\n",
    "W_conv3 = weight_variable([5,5,32,64])\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "W_conv4 = weight_variable([5,5,64,64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "W_conv5 = weight_variable([5,5,64,96])\n",
    "b_conv5 = bias_variable([96])\n",
    "\n",
    "W_conv6 = weight_variable([5,5,96,96])\n",
    "b_conv6 = bias_variable([96])\n",
    "\n",
    "W_conv7 = weight_variable([5,5,96,128])\n",
    "b_conv7 = bias_variable([128])\n",
    "\n",
    "W_conv8 = weight_variable([5,5,128,128])\n",
    "b_conv8 = bias_variable([128])\n",
    "\n",
    "# fully connected layers\n",
    "W_fc1 = weight_variable([8192,4096])\n",
    "b_fc1 = bias_variable([4096])\n",
    "\n",
    "W_fc2 = weight_variable([4096,4096])\n",
    "b_fc2 = bias_variable([4096])\n",
    "\n",
    "W_fc3 = weight_variable([4096,2])\n",
    "b_fc3 = bias_variable([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout to conv layers\n",
    "keep_prob_conv = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dropout to fc layers\n",
    "keep_prob_fc = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow model\n",
    "# conv layer 1\n",
    "h_conv1 = conv2d(X, W_conv1)\n",
    "h_conv1_activated = tf.nn.relu(h_conv1 + b_conv1)\n",
    "\n",
    "# conv layer 2\n",
    "h_conv2 = conv2d(h_conv1_activated, W_conv2)\n",
    "h_conv2_activated = tf.nn.relu(h_conv2 + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_activated)\n",
    "h_pool2_dropout = tf.nn.dropout(h_pool2, keep_prob_conv)\n",
    "\n",
    "# conv layer 3\n",
    "h_conv3 = conv2d(h_pool2_dropout, W_conv3)\n",
    "h_conv3_activated = tf.nn.relu(h_conv3 + b_conv3)\n",
    "\n",
    "# conv layer 4\n",
    "h_conv4 = conv2d(h_conv3_activated, W_conv4)\n",
    "h_conv4_activated = tf.nn.relu(h_conv4 + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4_activated)\n",
    "h_pool4_dropout = tf.nn.dropout(h_pool4, keep_prob_conv)\n",
    "\n",
    "# conv layer 5\n",
    "h_conv5 = conv2d(h_pool4_dropout, W_conv5)\n",
    "h_conv5_activated = tf.nn.relu(h_conv5 + b_conv5)\n",
    "\n",
    "#conv layer 6\n",
    "h_conv6 = conv2d(h_conv5_activated, W_conv6)\n",
    "h_conv6_activated = tf.nn.relu(h_conv6 + b_conv6)\n",
    "h_pool6 = max_pool_2x2(h_conv6_activated)\n",
    "h_pool6_dropout = tf.nn.dropout(h_pool6, keep_prob_conv)\n",
    "\n",
    "# conv layer 7\n",
    "h_conv7 = conv2d(h_pool6_dropout, W_conv7)\n",
    "h_conv7_activated = tf.nn.relu(h_conv7 + b_conv7)\n",
    "\n",
    "#conv layer 8\n",
    "h_conv8 = conv2d(h_conv7_activated, W_conv8)\n",
    "h_conv8_activated = tf.nn.relu(h_conv8 + b_conv8)\n",
    "h_pool8 = max_pool_2x2(h_conv8_activated)\n",
    "h_pool8_dropout = tf.nn.dropout(h_pool8, keep_prob_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output from conv layers\n",
    "x_fc = tf.reshape(h_pool8_dropout, shape=[-1, 8192])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully conected layer 1\n",
    "h_fc1 = tf.matmul(x_fc, W_fc1) + b_fc1\n",
    "h_fc1_activated = tf.nn.relu(h_fc1)\n",
    "h_fc1_dropout = tf.nn.dropout(h_fc1_activated, keep_prob_fc)\n",
    "\n",
    "# fully conected layer 2\n",
    "h_fc2 = tf.matmul(h_fc1_dropout, W_fc2)\n",
    "h_fc2_activated = tf.nn.relu(h_fc2)\n",
    "\n",
    "# fully conected layer 3\n",
    "y = tf.matmul(h_fc2_activated, W_fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cost function \n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the tensorflow computational graph to minimize the cost using adam optimizer\n",
    "train = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict output using our model\n",
    "# returns a boolean array\n",
    "predictions = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the boolean array to float and calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensorflow interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables inside the graph\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load graph\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"./model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save graph\n",
    "def save_graph(epoch):\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"./model_epoch_final.ckpt\", global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate train and test accuracy\n",
    "def calculate_accuracy():\n",
    "    # reset iterators\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(test_iterator.initializer)\n",
    "    \n",
    "    train_acc = 0\n",
    "    train_iters = 0\n",
    "    \n",
    "    # check train accuracy over 5 mini batches, to reduce computation\n",
    "    for _ in range(5):\n",
    "        train_iters += 1\n",
    "        X_test, y_test = sess.run([input_mini_batch, label_mini_batch])\n",
    "        train_acc += sess.run(accuracy, feed_dict={X: X_test, y_: y_test, keep_prob_fc: 1.0, keep_prob_conv: 1.0})\n",
    "\n",
    "    test_acc = 0\n",
    "    test_iters = 0\n",
    "    while True:\n",
    "        try:\n",
    "            test_iters += 1\n",
    "            X_test, y_test = sess.run([X_test_mini_batch, y_test_mini_batch])\n",
    "            test_acc += sess.run(accuracy, feed_dict={X: X_test, y_: y_test, keep_prob_fc: 1.0, keep_prob_conv: 1.0})\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "    # print accuracy\n",
    "    print(f\"Train accuracy: {train_acc/train_iters}\")\n",
    "    print(f\"Test accuracy: {test_acc/test_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train the model! (uses mini-batch)\n",
    "for epoch in range(0,n_epochs):\n",
    "    epoch_loss = 0\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            X_for_train, y_for_train = sess.run([input_mini_batch, label_mini_batch])\n",
    "            t, c = sess.run([train, cost], feed_dict={X: X_for_train, y_: y_for_train, keep_prob_fc: 0.5, keep_prob_conv: 0.8})\n",
    "            epoch_loss += c\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    # print loss\n",
    "    print(f\"Epoch {epoch} out of {n_epochs}, loss: {epoch_loss}\")\n",
    "    \n",
    "    # print train and test accuracies\n",
    "    calculate_accuracy()\n",
    "    \n",
    "    # Save Graph\n",
    "    if epoch % 2 == 0 and epoch >= 2:\n",
    "        save_graph(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate final accuracy\n",
    "sess.run(test_iterator.initializer)\n",
    "acc = 0\n",
    "iters = 0\n",
    "while True:\n",
    "    try:\n",
    "        iters += 1\n",
    "        X_for_train, y_for_train = sess.run([X_test_mini_batch, y_test_mini_batch])\n",
    "        acc += sess.run(accuracy, feed_dict={X: X_for_train, y_: y_for_train, keep_prob_fc: 1.0, keep_prob_conv: 1.0})\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "print(f\"Accuracy: {acc/iters}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
